{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928e9ecc-956e-4397-96c6-c1d9b6d29e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code trains an agent (which is a DNN) to mix a set of active particles \\nto mix a binary system of passive particles. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This code trains an agent (which is a DNN) to mix a set of active particles \n",
    "to mix a binary system of passive particles. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c326de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from math import pi, ceil, sqrt\n",
    "import time\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import ctypes\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if not os.path.exists(\"particle_data\"):os.makedirs(\"particle_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157e92e",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91019a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INT_RAD = 1.0 # Radius of interior particle\n",
    "DOMAIN_TO_RAD = 15 # Ratio of domain radius to interior particle radius\n",
    "INT_DIA = 2.0*INT_RAD  # Diameter of interior particle\n",
    "DOMAIN_RAD = DOMAIN_TO_RAD * INT_RAD  #  Radius of the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4efee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_rad = INT_RAD # Radius of a particle\n",
    "cell_width = 4*part_rad # single cell width for force calculation\n",
    "nn_cell_width = 4*part_rad # single cell width for mixing index calculation\n",
    "sq_cutoff_rad = (nn_cell_width)**2 # for the search radius r_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec6104",
   "metadata": {},
   "source": [
    "Declare C function prototypes for \"mixing.c\": \n",
    "To communicate with Particle Dynamic Subroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00d2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shared library to communicate with Particle Dynamic Sub-routine\n",
    "path = os.getcwd()\n",
    "mixing_lib = ctypes.CDLL(os.path.join(path,'mixing.so')) # '.so' is the compiled C file name\n",
    "\n",
    "# Define the C function signature\n",
    "process = mixing_lib.mix_state\n",
    "process.argtypes = [ctypes.POINTER(ctypes.c_double), ctypes.POINTER(ctypes.c_double),\\\n",
    "                    ctypes.c_int, ctypes.c_int, ctypes.c_double, ctypes.c_double,\\\n",
    "                    ctypes.c_int, ctypes.c_int, ctypes.c_int, ctypes.c_double]#, ctypes.c_double, ctypes.c_double]\n",
    "#state, thetaRL, rows, cols, P_rad, D_rad, N_wall, N_passive, N_active, delta_t, Time\n",
    "process.restype = ctypes.POINTER(ctypes.c_double)\n",
    "\n",
    "# To free the dynamic array in C code\n",
    "dealloc_mem = mixing_lib.free_array\n",
    "dealloc_mem.argtypes = [ctypes.POINTER(ctypes.c_double)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b28824",
   "metadata": {},
   "source": [
    "Function call for mixing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cd2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Particle Dynamic Sub-routine\n",
    "def call_mix_function(state_array, theta_array, del_t):\n",
    "    rows = state_array.shape[0] # No of rows \n",
    "    cols = state_array.shape[1]\n",
    "    \n",
    "    # Convert the state to a flat C-style array\n",
    "    state_array = state_array.astype(np.float64)\n",
    "    state_array_flat = state_array.flatten(order='C')\n",
    "    cstyle_state = state_array_flat.ctypes.data_as(ctypes.POINTER(ctypes.c_double))\n",
    "    \n",
    "    # Convert theta to a C-style array\n",
    "    theta_array = theta_array.astype(np.float64)\n",
    "    cstyle_theta = theta_array.ctypes.data_as(ctypes.POINTER(ctypes.c_double))\n",
    "\n",
    "    #Passing arguements to C function.................................................\n",
    "    new_state = process(cstyle_state, cstyle_theta, rows, cols, INT_RAD, DOMAIN_RAD,\\\n",
    "                        N_WALL, N_PASSIVE, N_ACTIVE, del_t)#,Time)\n",
    "    #...................................................................................\n",
    "    \n",
    "    # Convert the pointer to a Python array\n",
    "    python_state = np.ctypeslib.as_array(new_state, shape=(rows * cols,))\n",
    "    \n",
    "    # convert from 1D to 2D Python array\n",
    "    mod_state = python_state.reshape(rows,cols).copy()\n",
    "    \n",
    "    # Free the allocated memory in C\n",
    "    dealloc_mem(new_state)\n",
    "    \n",
    "    return mod_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5af21",
   "metadata": {},
   "source": [
    "Mixing Index Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "532bed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the Mixing Index\n",
    "def nn_fn(data_passive):\n",
    "    '''Data passive holds the x,y poitions for passive particles'''\n",
    "    nn_list = np.zeros(N_PASSIVE,dtype=int)\n",
    "    posx_nn = data_passive[:,0]\n",
    "    posy_nn = data_passive[:,1]\n",
    "    head_nn = np.ones(N_cells_nn)*(-1)\n",
    "    cell_x_nn = (np.divide(np.subtract(posx_nn,x_low_nn),nn_cell_width)).astype(int) # Putting agent into respective xcell\n",
    "    cell_y_nn = (np.divide(np.subtract(posy_nn,y_low_nn),nn_cell_width)).astype(int); # Putting agent into respective ycell\n",
    "    cell_pos_nn = (cell_x_nn*Nc_y_nn + cell_y_nn).astype(int); # Cell number calculated vertically\n",
    "    \n",
    "    #assigning particles' (identity) in head2 & nn_list for easily accessing\n",
    "    for i in range (N_PASSIVE):\n",
    "        nn_list[i] = head_nn[cell_pos_nn[i]]\n",
    "        head_nn[cell_pos_nn[i]] = i\n",
    "\n",
    "    # To save the number of type\n",
    "    pass_type = np.zeros((N_PASSIVE,2)) \n",
    "\n",
    "    for i in range (Nc_x_nn):\n",
    "        for j in range (Nc_y_nn):\n",
    "            nn_cell_n = i*Nc_y_nn + j\n",
    "            iat = int(head_nn[nn_cell_n])\n",
    "            while (iat != -1): # -1 implies no particles in this head/cell \n",
    "                for neigh_cell_x in [i-1,i,i+1]:\n",
    "                    for neigh_cell_y in [j-1,j,j+1]:\n",
    "                        neigh_cell_n = neigh_cell_x*Nc_y_nn + neigh_cell_y\n",
    "                        jat = int(head_nn[neigh_cell_n])\n",
    "                        while (jat != -1): # -1 implies no particles in this head/cell \n",
    "                            if(iat != jat):\n",
    "                                # Distance calculation of neighboring particles\n",
    "                                dx = posx_nn[jat] - posx_nn[iat]\n",
    "                                dy = posy_nn[jat] - posy_nn[iat]\n",
    "                                sq_distance = dx*dx + dy*dy\n",
    "                                \n",
    "                                if (sq_distance <=sq_cutoff_rad):\n",
    "                                    if(jat<PASSIVE_HALF):\n",
    "                                        pass_type[iat][0] += 1 # TYPE 1 PASSIVE\n",
    "                                    if(jat>=PASSIVE_HALF):\n",
    "                                        pass_type[iat][1] += 1 # TYPE 2 PASSIVE\n",
    "\n",
    "                            jat = int(nn_list[jat])\n",
    "\n",
    "                iat = nn_list[iat]\n",
    "                \n",
    "    ratio_1 = np.sum(2*np.nan_to_num(np.divide(pass_type[:PASSIVE_HALF,1],(pass_type[:PASSIVE_HALF,0]+pass_type[:PASSIVE_HALF,1]))),axis=0)\n",
    "    ratio_2 = np.sum(2*np.nan_to_num(np.divide(pass_type[-PASSIVE_HALF:,0],(pass_type[-PASSIVE_HALF:,0]+pass_type[-PASSIVE_HALF:,1]))),axis=0)\n",
    "    mix_index = (ratio_1+ratio_2)/N_PASSIVE #Mixing Index\n",
    "\n",
    "    return mix_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d0fc8",
   "metadata": {},
   "source": [
    "Functions in class Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51ea7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate:\n",
    "\n",
    "    \"\"\"To read initial particle positions and assign values to the arrays and variables used\"\"\"\n",
    "    @staticmethod\n",
    "    def interiorParticlesRead():\n",
    "        global theta, rad, vel, part_type,  N_WALL, N_PASSIVE, N_ACTIVE,\\\n",
    "            INT_PARTICLES, N_TOTAL, PASSIVE_HALF, x_low_nn, y_low_nn, x_max_nn,\\\n",
    "            y_max_nn, Nc_x_nn, Nc_y_nn, N_cells_nn\n",
    "        part = pd.read_csv(\"ini_state_q1.csv\", header = 0, delimiter=\"\\t\", names=[\"x\", \"y\",\\\n",
    "                        \"radius\", \"part_type\"])#Read values and assigned under these headers\n",
    "        pos_x = np.asarray(part['x']) #Assigns the values under x to an array named pos_x\n",
    "        pos_y = np.asarray(part['y']) #Assigns the values under y to an array named pos_y\n",
    "        rad = np.asarray(part['radius']).reshape(-1,1) # radius of all particles used\n",
    "        \n",
    "        '''Type of particle : 1:active,-1:passive, 0:boundary'''\n",
    "        part_type = np.asarray(part['part_type']).reshape(-1,1)\n",
    "\n",
    "        '''values needed to define neighbouring cells to search for neighbours \n",
    "           - helps to search for neighbours faster. used in mixing index calculation'''        \n",
    "        x_low_nn = np.amin(pos_x)-nn_cell_width\n",
    "        y_low_nn = np.amin(pos_y)-nn_cell_width\n",
    "        x_max_nn = np.amax(pos_x)+nn_cell_width\n",
    "        y_max_nn = np.amax(pos_y)+nn_cell_width\n",
    "        Nc_x_nn = int(np.ceil((x_max_nn-x_low_nn)/nn_cell_width))\n",
    "        Nc_y_nn = int(np.ceil((y_max_nn-y_low_nn)/nn_cell_width))\n",
    "        N_cells_nn = int(Nc_x_nn*Nc_y_nn) # Total no of cells\n",
    "        \n",
    "        N_WALL = len((part_type[part_type==0]))\n",
    "        N_PASSIVE = len((part_type[part_type==-1]))\n",
    "        N_ACTIVE = len((part_type[part_type==1]))\n",
    "        INT_PARTICLES = N_ACTIVE + N_PASSIVE\n",
    "        N_TOTAL = INT_PARTICLES + N_WALL\n",
    "        PASSIVE_HALF = int(N_PASSIVE/2)\n",
    "        del part # Memory freed\n",
    "    \n",
    "    '''Initial state : (x, y) of passive & active particles'''\n",
    "    def readInitialState():\n",
    "        \"\"\"\n",
    "        Ensure all data is entered through a single file in the format given below.\n",
    "        Ensure the data is in order of wall data, passive data and active data \n",
    "        from top to bottom.The initial input file contained u,v velocities from which \n",
    "        orientation (theta) is calculated.\n",
    "        \"\"\"\n",
    "        #active, passive, and wall data : x, y, rad, vel, theta, type : indices :0 1 2 3 4 5\n",
    "        global wall_state\n",
    "        part = pd.read_csv(\"ini_state_q1.csv\", delimiter=\"\\t\",header=0) \n",
    "        initial_state = part.to_numpy(dtype=float)\n",
    "        int_state = initial_state[:INT_PARTICLES, :2] # Storing initial position of active particles of active particles\n",
    "        wall_state = initial_state[INT_PARTICLES:, :2] # Storing wall particle x,y data\n",
    "        del part\n",
    "        return int_state # Returns initial state : (x, y) of passive & active particles   \n",
    "    \n",
    "    '''Receives a state, does dynamics through a series of sub functions and returns new state'''\n",
    "    @staticmethod\n",
    "    def systemState(interior_state, theta_RL, delta_t): # receives (x, y), theta(RL) data - for active+passive separate\n",
    "        global wall_state, rad, part_type\n",
    "        state = np.concatenate((interior_state,wall_state), axis=0) # active+passive+wall - x,y data\n",
    "        theta_up = np.concatenate((theta_RL, np.zeros(N_PASSIVE) , np.zeros(N_WALL)),axis=0).reshape(-1,1)\n",
    "        c_state = np.hstack((state, rad, part_type)) # x,y,rad,part_type for all particles\n",
    "\n",
    "        #......................................................\n",
    "        up_state = call_mix_function(c_state,theta_up,delta_t) # CALLING FUNCTION FOR MIXING\n",
    "        '''up_state is the updated state of the system after delta_t time'''\n",
    "        #......................................................\n",
    "         \n",
    "        '''rew is the reward or mixing index(MI) based on current position of passive particles'''\n",
    "        rew = nn_fn(up_state[N_ACTIVE:INT_PARTICLES,:2]) #passing passive positions to find MI\n",
    "    \n",
    "        obs_state = up_state[:INT_PARTICLES, :2] # x,y positions of both active + passsive\n",
    "        return obs_state, rew # Returning passive, active states separately along \n",
    "                            # with reward(MI) after delta_t time - delta_t/dt iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308d3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To initiate the variables and arrays used'''\n",
    "Simulate.interiorParticlesRead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8432a80",
   "metadata": {},
   "source": [
    "# RL Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d58ec9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pallab_workstation/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tqdm\n",
    "import rich\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import tensorflow as tf\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7699a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory particle_data for storing data\n",
    "if not os.path.exists(\"RL_particle_data\"):\n",
    "  os.makedirs(\"RL_particle_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788506b",
   "metadata": {},
   "source": [
    "Directories for Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "631153c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/PPO\"\n",
    "log_dir =  \"logs\"\n",
    "\n",
    "if not os.path.exists(\"RL_particle_data\"):  # Create directory particle_data for storing data\n",
    "  os.makedirs(\"RL_particle_data\")\n",
    "\n",
    "if not os.path.exists(models_dir):  # Create directory particle_data for storing data\n",
    "  os.makedirs(models_dir)\n",
    "if not os.path.exists(log_dir):  # Create directory particle_data for storing data\n",
    "  os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ae16f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_time = 50000.0 # Total Simulation Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c3d48",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2214fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''t_sim determines the time duration for an episode'''\n",
    "'''delta_t is the interval with in which RL interacts with C subroutine'''\n",
    "\n",
    "\"\"\"ParticleMixer is the Custom Environment that follows gym interface\"\"\"\n",
    "class ParticleMixer(gym.Env):\n",
    "\n",
    "    '''To initiate the variables and arrays in the environment, a constructor is used'''\n",
    "    def __init__(self, n_dim=2, t_sim=sim_time, delta_t=20.0, rr=0):  # self is a constructor\n",
    "        super(ParticleMixer, self).__init__()\n",
    "        self.n_obs = INT_PARTICLES  # Observing particle count\n",
    "        self.n_active = N_ACTIVE # No of active particles\n",
    "        \n",
    "        # Define the pool of angles\n",
    "        self.angle_pool = np.arange(0, 2.0*pi, pi/2) \n",
    "        '''pool of angles is in steps of pi/2 - can be replaced with the angle of our choice'''\n",
    "\n",
    "        # Setting up the action space                                                                                                            \n",
    "        self.action_space = gym.spaces.MultiDiscrete([len(self.angle_pool)] * self.n_active)\n",
    "\n",
    "        '''Defining the observation space that contains x,y for active and passive\n",
    "           Defined as a 1-D array'''\n",
    "        self.observation_space = spaces.Box(low=0, high=2*DOMAIN_RAD,\n",
    "                                            shape=((self.n_obs)*n_dim, ), dtype=np.float64)  # Observation space size: [n_passive+n_active] x 2 : (x,y)                                    \n",
    "\n",
    "        self.obs_state = np.zeros((self.n_obs,2), dtype=np.float64) # 2 columns (x,y)\n",
    "        self.time_statistics = np.zeros((5000,2), dtype=float) # To save episode no and length\n",
    "        self.t_sim = t_sim # Maximum simulation time for 1 episode\n",
    "        self.n_time = 0.0 # Current iteration\n",
    "        self.del_t = delta_t # Duration between agent-environment interaction\n",
    "        self.t = 0.0 # Current time\n",
    "        self.row=rr # To point to the row in time_statistics\n",
    "        self.prev_action = self.action_space.sample() # creates a sample action space\n",
    "        self.done = False # To determine if the episode is over. False:Not Over, True:Over\n",
    "\n",
    "    def step(self, action): #This function is called by alg after reset function to perform the action\n",
    "                            # This function is linked to the system dynamics equation\n",
    "        self.action_val = [self.angle_pool[i] for i in action]  # Sampled angles for active particles\n",
    "        #print(self.prev_action)\n",
    "        \n",
    "        \"\"\" To find & assign velocity when angle \"d_theta\" is the action_space \"\"\"\n",
    "\n",
    "        #self.active_state[:,2] = self.prev_action  # theta assigned\n",
    "\n",
    "        self.obs_state, index = Simulate.systemState(self.obs_state, self.action_val, self.del_t) # sending x, y, theta\n",
    "        \"\"\" Send current [position, theta] and returns new [position, theta] after implementing Langevin code for delta_t time\"\"\"\n",
    "        \n",
    "        '''We take the -ve of index as reward. As mixing is better, index value decreases\n",
    "        If we take the negative, the value can be seen increasing with mixing (in the -ve range towards 0)\n",
    "        As the algorithm tends to maximise reward, the cumulative reward should be closer to 0'''\n",
    "        reward = index \n",
    "        observation = self.obs_state.flatten() # self.observation_space.sample()\n",
    "        info = {}\n",
    "        self.t += self.del_t  # Total Time Completed\n",
    "        self.n_time += self.del_t/0.01  # Total Timesteps completed\n",
    "        if ((self.t > self.t_sim)|(reward>=0.99)): \n",
    "            self.done = True\n",
    "            self.time_statistics[self.row,0] = self.t\n",
    "            self.time_statistics[self.row,1] = reward\n",
    "            self.row = self.row+1\n",
    "        return observation, reward, self.done, info\n",
    "\n",
    "    def reset(self): #While running the RL algorithm, this function is called first. This sets initial observation space\n",
    "        self.t = 0.0\n",
    "        self.n_time = 0.0\n",
    "        self.done=False     \n",
    "        self.obs_state = Simulate.readInitialState() # Resetting the initial state before an episode during first case : (x, y, theta(from i/p file))\n",
    "        observation = self.obs_state.flatten() # 1-D initial state\n",
    "#         print(\"1 reset\")\n",
    "        return observation  # reward, done, info can't be included\n",
    "    def render(self):\n",
    "        pass\n",
    "    def close (self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b681630",
   "metadata": {},
   "source": [
    "Assigning environment to variable env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03c1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParticleMixer() # env is the environment now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f20e3",
   "metadata": {},
   "source": [
    "Check the custom environment for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd293fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85533f44",
   "metadata": {},
   "source": [
    "Algorithm and Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42eb8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e9b9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MLP policy with Relu activation function\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=[512,256,64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dab3d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hyperparameters = {\n",
    "    'learning_rate': 0.000001,\n",
    "    # Add other hyperparameters and their new values here separeted by commas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38bcdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/PPO\" # Directory for saving the models\n",
    "log_dir =  \"logs\" # Directory for saving the data for tensor board view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba6d348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model_ppo = PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs, **new_hyperparameters, tensorboard_log=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cf9f2",
   "metadata": {},
   "source": [
    "Learning happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9bfad99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs/PPO_0\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 4    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 413  |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | 1.84e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4            |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 821          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023084946 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -4.15        |\n",
      "|    explained_variance   | -0.165       |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.762        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 27           |\n",
      "------------------------------------------\n",
      "Logging to logs/PPO_0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m learn_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m300\u001b[39m):\u001b[38;5;66;03m# repeat the total_timesteps 300 times\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel_ppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     model_ppo\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Saving the trained model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    244\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 248\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:175\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[1;32m    173\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 175\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 54\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn [20], line 44\u001b[0m, in \u001b[0;36mParticleMixer.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\" To find & assign velocity when angle \"d_theta\" is the action_space \"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#self.active_state[:,2] = self.prev_action  # theta assigned\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_state, index \u001b[38;5;241m=\u001b[39m \u001b[43mSimulate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystemState\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdel_t\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# sending x, y, theta\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\" Send current [position, theta] and returns new [position, theta] after implementing Langevin code for delta_t time\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m'''We take the -ve of index as reward. As mixing is better, index value decreases\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mIf we take the negative, the value can be seen increasing with mixing (in the -ve range towards 0)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mAs the algorithm tends to maximise reward, the cumulative reward should be closer to 0'''\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [13], line 62\u001b[0m, in \u001b[0;36mSimulate.systemState\u001b[0;34m(interior_state, theta_RL, delta_t)\u001b[0m\n\u001b[1;32m     59\u001b[0m c_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((state, rad, part_type)) \u001b[38;5;66;03m# x,y,rad,part_type for all particles\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#......................................................\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m up_state \u001b[38;5;241m=\u001b[39m \u001b[43mcall_mix_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtheta_up\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# CALLING FUNCTION FOR MIXING\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m'''up_state is the updated state of the system after delta_t time'''\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#......................................................\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [10], line 16\u001b[0m, in \u001b[0;36mcall_mix_function\u001b[0;34m(state_array, theta_array, del_t)\u001b[0m\n\u001b[1;32m     13\u001b[0m cstyle_theta \u001b[38;5;241m=\u001b[39m theta_array\u001b[38;5;241m.\u001b[39mctypes\u001b[38;5;241m.\u001b[39mdata_as(ctypes\u001b[38;5;241m.\u001b[39mPOINTER(ctypes\u001b[38;5;241m.\u001b[39mc_double))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Passing arguements to C function.................................................\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcstyle_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcstyle_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINT_RAD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDOMAIN_RAD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mN_WALL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_PASSIVE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_ACTIVE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdel_t\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#,Time)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#...................................................................................\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert the pointer to a Python array\u001b[39;00m\n\u001b[1;32m     21\u001b[0m python_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mctypeslib\u001b[38;5;241m.\u001b[39mas_array(new_state, shape\u001b[38;5;241m=\u001b[39m(rows \u001b[38;5;241m*\u001b[39m cols,))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Intermediate models are saved after every 4096 agent-env communications'''\n",
    "learn_start = time.time()\n",
    "for i in range(300):# repeat the total_timesteps 300 times\n",
    "    model_ppo.learn(total_timesteps=4096, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model_ppo.save(f\"{models_dir}/{i+1}\") # Saving the trained model\n",
    "    if (i+1)%100==0:\n",
    "        outfile = \"maxtime_vs_maxMI_statistics.csv\"\n",
    "        df = pd.DataFrame(env.time_statistics,columns=['max_time','max_MI'])\n",
    "        df.to_csv(outfile, index=False, sep='\\t')\n",
    "print(\"learn_time = \",time.time()-learn_start) # Printing time taken to learn\n",
    "\n",
    "'''\n",
    "the policy is updated after every \"n_steps\" iterations - 2048 default (agent-env communications)\n",
    "the model is saved after every \"total_timesteps\" iterations and \n",
    "continue to iterate until for loop is over -->final time_steps = 300*4096\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7446726",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save the episode length for each episode along with MI'''\n",
    "outfile = \"maxtime_vs_maxMI_statistics_final.csv\"\n",
    "df = pd.DataFrame(env.time_statistics,columns=['max_time','max_MI'])\n",
    "df.to_csv(outfile, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d8f9a-2bc2-443e-98fb-fc3826c2c1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
